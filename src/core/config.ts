import { config } from "dotenv";
import { resolve, dirname } from "path";
import { fileURLToPath } from "url";
import { getSettings, type Settings } from "./settings.js";
import { log } from "../logger.js";

// è·å– YTerm å®‰è£…ç›®å½•ï¼ˆå³æœ¬é¡¹ç›®æ ¹ç›®å½•ï¼‰
// ESM ä¸­ä½¿ç”¨ import.meta.url æ›¿ä»£ __dirname
const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);
const YTerm_ROOT = resolve(__dirname, "..");

// åŠ è½½ç¯å¢ƒå˜é‡ï¼Œä¼˜å…ˆçº§ï¼ˆä»é«˜åˆ°ä½ï¼‰ï¼š
// 1. settings.json (æœ€é«˜ä¼˜å…ˆçº§ï¼Œåœ¨ getSettings() ä¸­å¤„ç†)
// 2. å½“å‰å·¥ä½œç›®å½•çš„ .env.local
// 3. å½“å‰å·¥ä½œç›®å½•çš„ .env
// 4. YTerm å®‰è£…ç›®å½•çš„ .env.local
// 5. YTerm å®‰è£…ç›®å½•çš„ .env
// dotenv ä¸ä¼šè¦†ç›–å·²å­˜åœ¨çš„ç¯å¢ƒå˜é‡ï¼Œæ‰€ä»¥æŒ‰ä¼˜å…ˆçº§ä»é«˜åˆ°ä½åŠ è½½
config({ path: resolve(process.cwd(), ".env.local"), quiet: true });
config({ path: resolve(process.cwd(), ".env"), quiet: true });
config({ path: resolve(YTerm_ROOT, ".env.local"), quiet: true });
config({ path: resolve(YTerm_ROOT, ".env"), quiet: true });

// æ¨¡å‹ç±»å‹æšä¸¾
export enum ModelType {
  LOCAL = "local",
  CLOUD = "cloud",
}

// Provider ç±»å‹æšä¸¾
export enum ProviderType {
  OLLAMA = "OLLAMA",
  OPENROUTER = "OPENROUTER",
  OPENAI = "OPENAI",
  ANTHROPIC = "ANTHROPIC",
}

export interface ModelConfig {
  name: string;
  model: string;
  type: ModelType;
  description?: string;
  supportsTools?: boolean; // æ˜¯å¦æ”¯æŒå·¥å…·è°ƒç”¨
  contextWindow?: number;  // ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼ˆtokensï¼‰
  provider?: ProviderType; // æ¨¡å‹æä¾›è€…
}

// ============ é…ç½®è®¿é—®å™¨ï¼ˆä» settings è·å–ï¼‰ ============

// è·å–å½“å‰ Provider
export function getUseProvider(): ProviderType {
  return getSettings().provider as ProviderType;
}

// è·å–å½“å‰é»˜è®¤æ¨¡å‹ï¼ˆæ ¹æ®å½“å‰ provider åŠ¨æ€è·å–ï¼‰
export function getDefaultModel(): string {
  const settings = getSettings();
  
  switch (settings.provider) {
    case "OLLAMA":
      return settings.ollama.model;
    case "OPENROUTER":
      return settings.openRouter.model;
    case "OPENAI":
      return settings.openAI.model;
    case "ANTHROPIC":
      return settings.anthropic.model;
    default:
      return settings.ollama.model;
  }
}

// Ollama é…ç½®
export function getOllamaHost(): string {
  return getSettings().ollama.host;
}

export function getOllamaCloudHost(): string {
  return getSettings().ollama.cloudHost;
}

export function getOllamaCloudApiKey(): string {
  return getSettings().ollama.cloudApiKey;
}

// OpenRouter é…ç½®
export function getOpenRouterApiKey(): string {
  return getSettings().openRouter.apiKey;
}

export function getOpenRouterModel(): string {
  return getSettings().openRouter.model;
}

export function getOpenRouterContextLength(): number {
  return getSettings().openRouter.contextLength;
}

// OpenAI é…ç½®
export function getOpenAIApiKey(): string {
  return getSettings().openAI.apiKey;
}

export function getOpenAIBaseUrl(): string {
  return getSettings().openAI.baseUrl;
}

export function getOpenAIModel(): string {
  return getSettings().openAI.model;
}

export function getOpenAIContextLength(): number {
  return getSettings().openAI.contextLength;
}

// Anthropic é…ç½®
export function getAnthropicApiKey(): string {
  return getSettings().anthropic.apiKey;
}

export function getAnthropicBaseUrl(): string {
  return getSettings().anthropic.baseUrl;
}

export function getAnthropicModel(): string {
  return getSettings().anthropic.model;
}

export function getAnthropicContextLength(): number {
  return getSettings().anthropic.contextLength;
}

// ============ å¯¼å‡º ============

// ä½¿ç”¨ getter å‡½æ•°ï¼Œè¿™æ ·å¯ä»¥åŠ¨æ€è·å–æœ€æ–°è®¾ç½®
export const USE_PROVIDER = getUseProvider();
// æ³¨æ„ï¼šOLLAMA_MODEL_NAME ç°åœ¨è¡¨ç¤ºå½“å‰ä½¿ç”¨çš„æ¨¡å‹ï¼ˆæ ¹æ® provider åŠ¨æ€å˜åŒ–ï¼‰
export const CURRENT_MODEL_NAME = getDefaultModel();
export const OLLAMA_HOST = getOllamaHost();
export const OLLAMA_CLOUD_HOST = getOllamaCloudHost();
export const OLLAMA_CLOUD_API_KEY = getOllamaCloudApiKey();
export const OPENROUTER_API_KEY = getOpenRouterApiKey();
export const OPENROUTER_MODEL_NAME = getOpenRouterModel();
export const OPENROUTER_MODEL_CONTEXT_LENGTH = getOpenRouterContextLength();
export const OPENAI_API_KEY = getOpenAIApiKey();
export const OPENAI_MODEL_NAME = getOpenAIModel();
export const OPENAI_MODEL_CONTEXT_LENGTH = getOpenAIContextLength();
export const ANTHROPIC_API_KEY = getAnthropicApiKey();
export const ANTHROPIC_MODEL_NAME = getAnthropicModel();
export const ANTHROPIC_MODEL_CONTEXT_LENGTH = getAnthropicContextLength();

// é»˜è®¤ä¸Šä¸‹æ–‡çª—å£å¤§å°
export const DEFAULT_CONTEXT_WINDOW = 32768;

// ============ åŠ¨æ€ Ollama æ¨¡å‹ ============
// Ollama æ¨¡å‹é€šè¿‡ API åŠ¨æ€è·å–ï¼Œä¸å†ç¡¬ç¼–ç 

// æœ¬åœ° Ollama æ¨¡å‹ï¼ˆåŠ¨æ€å¡«å……ï¼‰
export let LOCAL_MODELS: ModelConfig[] = [];

// äº‘ç«¯ Ollama æ¨¡å‹ï¼ˆåŠ¨æ€å¡«å……ï¼‰
export let CLOUD_MODELS: ModelConfig[] = [];

// ============ å…¶ä»– Provider æ¨¡å‹é…ç½® ============

// OpenRouter æ¨¡å‹ï¼ˆåŠ¨æ€è·å–ï¼‰
export let OPENROUTER_MODELS: ModelConfig[] = [];

// OpenAI æ¨¡å‹ï¼ˆåŠ¨æ€è·å–ï¼‰
export let OPENAI_MODELS: ModelConfig[] = [];

// Anthropic æ¨¡å‹ï¼ˆé™æ€åˆ—è¡¨ï¼Œä½†ä½¿ç”¨ç¼“å­˜ï¼‰
export let ANTHROPIC_MODELS: ModelConfig[] = [];

// æ‰€æœ‰å¯ç”¨æ¨¡å‹ï¼ˆä¼šåœ¨åˆå§‹åŒ–åæ›´æ–°ï¼‰
export let ALL_MODELS: ModelConfig[] = [];

// æ”¯æŒå·¥å…·è°ƒç”¨çš„æ¨¡å‹
export function getToolCapableModels(): ModelConfig[] {
  return ALL_MODELS.filter((m) => m.supportsTools);
}

// ============ åˆå§‹åŒ–å‡½æ•° ============

import {
  getOllamaModelsWithCache,
  getCachedModelsSync,
  refreshModelsInBackground,
  type ParsedOllamaModel,
} from "./services/ollama.js";

import {
  getOpenRouterModelsWithCache,
  getCachedOpenRouterModelsSync,
  refreshOpenRouterModelsInBackground,
  type ParsedOpenRouterModel,
} from "./services/openrouter.js";

import {
  getOpenAIModelsWithCache,
  getCachedOpenAIModelsSync,
  refreshOpenAIModelsInBackground,
  type ParsedOpenAIModel,
} from "./services/openai.js";

import {
  getAnthropicModelsWithCache,
  getCachedAnthropicModelsSync,
  refreshAnthropicModelsInBackground,
  type ParsedAnthropicModel,
} from "./services/anthropic.js";

/**
 * å°† ParsedOllamaModel è½¬æ¢ä¸º ModelConfig
 */
function toModelConfig(parsed: ParsedOllamaModel): ModelConfig {
  return {
    name: parsed.name,
    model: parsed.model,
    type: parsed.isCloud ? ModelType.CLOUD : ModelType.LOCAL,
    description: parsed.description,
    supportsTools: parsed.supportsTools,
    contextWindow: parsed.contextWindow,
    provider: ProviderType.OLLAMA,
  };
}

/**
 * å°† ParsedOpenRouterModel è½¬æ¢ä¸º ModelConfig
 */
function toOpenRouterModelConfig(parsed: ParsedOpenRouterModel): ModelConfig {
  return {
    name: parsed.name,
    model: parsed.model,
    type: ModelType.CLOUD,
    description: parsed.description,
    supportsTools: parsed.supportsTools,
    contextWindow: parsed.contextWindow,
    provider: ProviderType.OPENROUTER,
  };
}

/**
 * å°† ParsedOpenAIModel è½¬æ¢ä¸º ModelConfig
 */
function toOpenAIModelConfig(parsed: ParsedOpenAIModel): ModelConfig {
  return {
    name: parsed.name,
    model: parsed.model,
    type: ModelType.CLOUD,
    description: parsed.description,
    supportsTools: parsed.supportsTools,
    contextWindow: parsed.contextWindow,
    provider: ProviderType.OPENAI,
  };
}

/**
 * å°† ParsedAnthropicModel è½¬æ¢ä¸º ModelConfig
 */
function toAnthropicModelConfig(parsed: ParsedAnthropicModel): ModelConfig {
  return {
    name: parsed.name,
    model: parsed.model,
    type: ModelType.CLOUD,
    description: parsed.description,
    supportsTools: parsed.supportsTools,
    contextWindow: parsed.contextWindow,
    provider: ProviderType.ANTHROPIC,
  };
}



/**
 * æ›´æ–°æ¨¡å‹åˆ—è¡¨
 */
function updateModelLists(
  local: ParsedOllamaModel[], 
  cloud: ParsedOllamaModel[],
  openRouterModels: ParsedOpenRouterModel[],
  openAIModels: ParsedOpenAIModel[],
  anthropicModels: ParsedAnthropicModel[]
): void {
  LOCAL_MODELS = local.map(toModelConfig);
  CLOUD_MODELS = cloud.map(toModelConfig);
  OPENROUTER_MODELS = openRouterModels.map(toOpenRouterModelConfig);
  OPENAI_MODELS = openAIModels.map(toOpenAIModelConfig);
  ANTHROPIC_MODELS = anthropicModels.map(toAnthropicModelConfig);
  
  ALL_MODELS = [...LOCAL_MODELS, ...CLOUD_MODELS, ...OPENROUTER_MODELS, ...OPENAI_MODELS, ...ANTHROPIC_MODELS];
  
  log.debug("All models updated", {
    local: LOCAL_MODELS.length,
    cloud: CLOUD_MODELS.length,
    openRouter: OPENROUTER_MODELS.length,
    openAI: OPENAI_MODELS.length,
    anthropic: ANTHROPIC_MODELS.length,
    total: ALL_MODELS.length,
  });
}

/**
 * å¿«é€Ÿåˆå§‹åŒ–æ¨¡å‹é…ç½®ï¼ˆä½¿ç”¨ç¼“å­˜ï¼Œä¸é˜»å¡ï¼‰
 * ç”¨äºå¯åŠ¨æ—¶å¿«é€ŸåŠ è½½ï¼Œåå°åˆ·æ–°ç¼“å­˜
 */
export function initializeModelsSync(): void {
  // æ£€æŸ¥ç¼“å­˜
  const cachedOllama = getCachedModelsSync();
  const cachedOpenRouter = getCachedOpenRouterModelsSync();
  const cachedOpenAI = getCachedOpenAIModelsSync();
  const cachedAnthropic = getCachedAnthropicModelsSync();

  if (cachedOllama || cachedOpenRouter || cachedOpenAI || cachedAnthropic) {
    updateModelLists(
      cachedOllama?.local || [],
      cachedOllama?.cloud || [],
      cachedOpenRouter || [],
      cachedOpenAI || [],
      cachedAnthropic || []
    );
    // åå°åˆ·æ–°ç¼“å­˜
    refreshModelsInBackground();
    refreshOpenRouterModelsInBackground();
    refreshOpenAIModelsInBackground();
    refreshAnthropicModelsInBackground();
  } else {
    // æ²¡æœ‰ç¼“å­˜æ—¶ï¼Œåå°è·å–ï¼ˆå¯åŠ¨æ—¶ä¸é˜»å¡ï¼‰
    refreshModelsInBackground();
    refreshOpenRouterModelsInBackground();
    refreshOpenAIModelsInBackground();
    refreshAnthropicModelsInBackground();
  }
}

/**
 * åˆå§‹åŒ–æ¨¡å‹é…ç½®ï¼ˆå¼‚æ­¥ï¼Œç­‰å¾…å®Œæˆï¼‰
 * ç”¨äºéœ€è¦å®Œæ•´æ¨¡å‹åˆ—è¡¨çš„åœºæ™¯ï¼ˆå¦‚ --listï¼‰
 */
export async function initializeModels(): Promise<void> {
  try {
    // å¹¶è¡Œè·å–éœ€è¦åŠ¨æ€è·å–çš„æ¨¡å‹
    const [ollamaResult, openRouterModels, openAIModels, anthropicModels] = await Promise.all([
      getOllamaModelsWithCache(),
      getOpenRouterModelsWithCache(),
      getOpenAIModelsWithCache(),
      getAnthropicModelsWithCache(),
    ]);

    updateModelLists(
      ollamaResult.local,
      ollamaResult.cloud,
      openRouterModels,
      openAIModels,
      anthropicModels
    );

    console.log(`Loaded models:`);
    console.log(`  Ollama: ${LOCAL_MODELS.length} local + ${CLOUD_MODELS.length} cloud`);
    console.log(`  OpenRouter: ${OPENROUTER_MODELS.length}`);
    console.log(`  OpenAI: ${OPENAI_MODELS.length}`);
    console.log(`  Anthropic: ${ANTHROPIC_MODELS.length}`);
    console.log(`  Total: ${ALL_MODELS.length} models`);
  } catch (error: any) {
    console.error("Failed to initialize models:", error.message);
  }
}

/**
 * åˆ·æ–°æ‰€æœ‰æ¨¡å‹åˆ—è¡¨
 */
export async function refreshAllModels(): Promise<void> {
  try {
    const { clearOllamaModelCache } = await import("./services/ollama.js");
    const { clearOpenRouterModelCache } = await import("./services/openrouter.js");
    const { clearOpenAIModelCache } = await import("./services/openai.js");
    const { clearAnthropicModelCache } = await import("./services/anthropic.js");

    // æ¸…é™¤æ‰€æœ‰æœåŠ¡çš„ç¼“å­˜
    clearOllamaModelCache();
    clearOpenRouterModelCache();
    clearOpenAIModelCache();
    clearAnthropicModelCache();

    // é‡æ–°åˆå§‹åŒ–
    await initializeModels();
    
    console.log("All models refreshed successfully");
  } catch (error: any) {
    console.error("Failed to refresh models:", error.message);
  }
}

/**
 * åˆ·æ–° Ollama æ¨¡å‹åˆ—è¡¨ï¼ˆå‘åå…¼å®¹ï¼‰
 */
export async function refreshOllamaModels(): Promise<void> {
  await refreshAllModels();
}

// ============ æ¨¡å‹æŸ¥è¯¢å‡½æ•° ============

// æ ¹æ®åç§°è·å–æ¨¡å‹é…ç½®
// ä¼˜å…ˆåŒ¹é…å½“å‰ Provider ä¸‹çš„æ¨¡å‹ï¼Œé¿å…è·¨ Provider çš„åç§°å†²çª
export function getModelConfig(name: string): ModelConfig | undefined {
  const currentProvider = getUseProvider();

  // å…ˆåœ¨å½“å‰ Provider çš„æ¨¡å‹ä¸­æŸ¥æ‰¾
  const currentProviderModels = ALL_MODELS.filter(m => m.provider === currentProvider);
  const matchInProvider = currentProviderModels.find((m) => m.name === name || m.model === name);
  if (matchInProvider) {
    return matchInProvider;
  }

  // åœ¨æ‰€æœ‰æ¨¡å‹ä¸­æŸ¥æ‰¾
  const matchInAll = ALL_MODELS.find((m) => m.name === name || m.model === name);
  if (matchInAll) {
    return matchInAll;
  }

  // å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œåˆ›å»ºåŠ¨æ€é…ç½®
  // è¿™å…è®¸ç”¨æˆ·ä½¿ç”¨ä»»æ„æ¨¡å‹åç§°ï¼Œå¹¶è‡ªåŠ¨ç»‘å®šåˆ°å½“å‰ Provider
  return {
    name,
    model: name,
    type: ModelType.CLOUD,
    description: `${currentProvider} - ${name}`,
    supportsTools: true,
    contextWindow: DEFAULT_CONTEXT_WINDOW,
    provider: currentProvider,
  };
}

// æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒå·¥å…·è°ƒç”¨
export function supportsToolCalling(name: string): boolean {
  const config = getModelConfig(name);
  return config?.supportsTools ?? false;
}

// è·å–æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£å¤§å°
export function getModelContextWindow(name: string): number {
  const config = getModelConfig(name);
  return config?.contextWindow ?? DEFAULT_CONTEXT_WINDOW;
}

// åˆ—å‡ºæ‰€æœ‰æ¨¡å‹
export function listModels(): void {
  const currentProvider = getUseProvider();
  const currentModel = getDefaultModel();
  console.log(`\nProvider: ${currentProvider}`);
  console.log(`Default Model: ${currentModel}`);
  console.log(`\nAvailable Models:`);

  const printModels = (models: ModelConfig[], category: string) => {
    if (models.length === 0) return;
    console.log(`\n[${category}]`);
    models.forEach((m) => {
      const toolIcon = m.supportsTools ? "ğŸ”§" : "  ";
      const ctx = m.contextWindow ? ` (${Math.round(m.contextWindow / 1000)}K ctx)` : "";
      console.log(`  ${toolIcon} ${m.name.padEnd(25)} ${m.description || m.model}${ctx}`);
    });
  };

  printModels(LOCAL_MODELS, "Ollama Local");
  printModels(CLOUD_MODELS, "Ollama Cloud");
  printModels(OPENROUTER_MODELS, "OpenRouter");
  printModels(OPENAI_MODELS, "OpenAI");
  printModels(ANTHROPIC_MODELS, "Anthropic");

  console.log("\nğŸ”§ = supports tool calling");
  console.log("\nTotal Models:");
  console.log(`  Local: ${LOCAL_MODELS.length}`);
  console.log(`  Cloud: ${CLOUD_MODELS.length + OPENROUTER_MODELS.length + OPENAI_MODELS.length + ANTHROPIC_MODELS.length}`);
  console.log(`  All: ${ALL_MODELS.length}\n`);
}
