import { config } from "dotenv";
import { resolve, dirname } from "path";
import { fileURLToPath } from "url";
import { ui } from "./ui.js";

// è·å– yterm å®‰è£…ç›®å½•ï¼ˆå³æœ¬é¡¹ç›®æ ¹ç›®å½•ï¼‰
// ESM ä¸­ä½¿ç”¨ import.meta.url æ›¿ä»£ __dirname
const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);
const YTERM_ROOT = resolve(__dirname, "..");

// åŠ è½½ç¯å¢ƒå˜é‡ï¼Œä¼˜å…ˆçº§ï¼ˆä»é«˜åˆ°ä½ï¼‰ï¼š
// 1. å½“å‰å·¥ä½œç›®å½•çš„ .env.local
// 2. å½“å‰å·¥ä½œç›®å½•çš„ .env
// 3. yterm å®‰è£…ç›®å½•çš„ .env.local
// 4. yterm å®‰è£…ç›®å½•çš„ .env
// dotenv ä¸ä¼šè¦†ç›–å·²å­˜åœ¨çš„ç¯å¢ƒå˜é‡ï¼Œæ‰€ä»¥æŒ‰ä¼˜å…ˆçº§ä»é«˜åˆ°ä½åŠ è½½
config({ path: resolve(process.cwd(), ".env.local"), quiet: true });
config({ path: resolve(process.cwd(), ".env"), quiet: true });
config({ path: resolve(YTERM_ROOT, ".env.local"), quiet: true });
config({ path: resolve(YTERM_ROOT, ".env"), quiet: true });

// æ¨¡å‹ç±»å‹æšä¸¾
export enum ModelType {
  LOCAL = "local",
  CLOUD = "cloud",
}

// Provider ç±»å‹æšä¸¾
export enum ProviderType {
  OLLAMA = "OLLAMA",
  OPENROUTER = "OPENROUTER",
  OPENAI = "OPENAI",
  ANTHROPIC = "ANTHROPIC",
}

export interface ModelConfig {
  name: string;
  model: string;
  type: ModelType;
  description?: string;
  supportsTools?: boolean; // æ˜¯å¦æ”¯æŒå·¥å…·è°ƒç”¨
  contextWindow?: number;  // ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼ˆtokensï¼‰
  provider?: ProviderType; // æ¨¡å‹æä¾›è€…
}

// Provider é…ç½®
export const USE_PROVIDER = (process.env.USE_PROVIDER || "OLLAMA").toUpperCase() as ProviderType;

// Ollama é…ç½®
export const OLLAMA_HOST = process.env.OLLAMA_HOST || "http://localhost:11434";
export const OLLAMA_CLOUD_HOST = process.env.OLLAMA_CLOUD_HOST || "https://ollama.com";
export const OLLAMA_CLOUD_API_KEY = process.env.OLLAMA_CLOUD_API_KEY || process.env.OLLAMA_API_KEY || "";

// OpenRouter é…ç½®
export const OPENROUTER_API_KEY = process.env.OPENROUTER_API_KEY || "";
export const OPENROUTER_MODEL_NAME = process.env.OPENROUTER_MODEL_NAME || "x-ai/grok-2-1212";
export const OPENROUTER_MODEL_CONTEXT_LENGTH = Number(process.env.OPENROUTER_MODEL_CONTEXT_LENGTH) || 131072;

// OpenAI é…ç½®
export const OPENAI_API_KEY = process.env.OPENAI_API_KEY || "";
export const OPENAI_MODEL_NAME = process.env.OPENAI_MODEL_NAME || "gpt-4o";
export const OPENAI_MODEL_CONTEXT_LENGTH = Number(process.env.OPENAI_MODEL_CONTEXT_LENGTH) || 128000;

// Anthropic é…ç½®
export const ANTHROPIC_API_KEY = process.env.ANTHROPIC_API_KEY || "";
export const ANTHROPIC_MODEL_NAME = process.env.ANTHROPIC_MODEL_NAME || "claude-sonnet-4-20250514";
export const ANTHROPIC_MODEL_CONTEXT_LENGTH = Number(process.env.ANTHROPIC_MODEL_CONTEXT_LENGTH) || 200000;

// é»˜è®¤ä¸Šä¸‹æ–‡çª—å£å¤§å°
export const DEFAULT_CONTEXT_WINDOW = 32768;

// æœ¬åœ°å®‰è£…çš„ Ollama æ¨¡å‹
export const LOCAL_MODELS: ModelConfig[] = [
  { name: "qwen3:4b", model: "qwen3:4b", type: ModelType.LOCAL, description: "Qwen3 4B - è½»é‡çº§", supportsTools: true, contextWindow: 32768, provider: ProviderType.OLLAMA },
  { name: "qwen3:8b", model: "qwen3:8b", type: ModelType.LOCAL, description: "Qwen3 8B", supportsTools: true, contextWindow: 32768, provider: ProviderType.OLLAMA },
  { name: "qwen3:0.6b", model: "qwen3:0.6b", type: ModelType.LOCAL, description: "Qwen3 0.6B - æœ€å°", supportsTools: false, contextWindow: 8192, provider: ProviderType.OLLAMA },
  { name: "qwen3-coder", model: "qwen3-coder:latest", type: ModelType.LOCAL, description: "Qwen3 Coder 18GB", supportsTools: true, contextWindow: 32768, provider: ProviderType.OLLAMA },
  { name: "gemma3:4b", model: "gemma3:4b", type: ModelType.LOCAL, description: "Gemma3 4B", supportsTools: false, contextWindow: 8192, provider: ProviderType.OLLAMA },
];

// Ollama Cloud æ¨¡å‹ï¼ˆæ”¯æŒ Tool Callingï¼‰
export const CLOUD_MODELS: ModelConfig[] = [
  { name: "gpt-oss", model: "gpt-oss:120b-cloud", type: ModelType.CLOUD, description: "GPT-OSS 120B - æ¨ç† & Agent", supportsTools: true, contextWindow: 128000, provider: ProviderType.OLLAMA },
  { name: "qwen3-coder-480b", model: "qwen3-coder:480b-cloud", type: ModelType.CLOUD, description: "Qwen3 Coder 480B - ç¼–ç ä¸“ç”¨", supportsTools: true, contextWindow: 128000, provider: ProviderType.OLLAMA },
  { name: "qwen3-vl", model: "qwen3-vl:235b-cloud", type: ModelType.CLOUD, description: "Qwen3 VL 235B - è§†è§‰è¯­è¨€", supportsTools: true, contextWindow: 160000, provider: ProviderType.OLLAMA },
  { name: "qwen3-vl-instruct", model: "qwen3-vl:235b-instruct-cloud", type: ModelType.CLOUD, description: "Qwen3 VL Instruct 235B - è§†è§‰è¯­è¨€", supportsTools: true, contextWindow: 160000, provider: ProviderType.OLLAMA },
  { name: "deepseek-v3", model: "deepseek-v3.1:671b-cloud", type: ModelType.CLOUD, description: "DeepSeek V3.1 671B - æ€è€ƒæ¨ç†", supportsTools: true, contextWindow: 160000, provider: ProviderType.OLLAMA },
  { name: "minimax-m2", model: "minimax-m2:cloud", type: ModelType.CLOUD, description: "MiniMax M2 Cloud", supportsTools: true, contextWindow: 200000, provider: ProviderType.OLLAMA },
  { name: "glm-4.6", model: "glm-4.6:cloud", type: ModelType.CLOUD, description: "GLM 4.6 Cloud", supportsTools: true, contextWindow: 198000, provider: ProviderType.OLLAMA },
];

// OpenRouter æ¨¡å‹
export const OPENROUTER_MODELS: ModelConfig[] = [
  { name: "openrouter", model: OPENROUTER_MODEL_NAME, type: ModelType.CLOUD, description: `OpenRouter - ${OPENROUTER_MODEL_NAME}`, supportsTools: true, contextWindow: OPENROUTER_MODEL_CONTEXT_LENGTH, provider: ProviderType.OPENROUTER },
];

// OpenAI æ¨¡å‹
export const OPENAI_MODELS: ModelConfig[] = [
  { name: "gpt-4o", model: OPENAI_MODEL_NAME, type: ModelType.CLOUD, description: `OpenAI - ${OPENAI_MODEL_NAME}`, supportsTools: true, contextWindow: OPENAI_MODEL_CONTEXT_LENGTH, provider: ProviderType.OPENAI },
  { name: "gpt-4o-mini", model: "gpt-4o-mini", type: ModelType.CLOUD, description: "OpenAI GPT-4o Mini", supportsTools: true, contextWindow: 128000, provider: ProviderType.OPENAI },
  { name: "gpt-4-turbo", model: "gpt-4-turbo", type: ModelType.CLOUD, description: "OpenAI GPT-4 Turbo", supportsTools: true, contextWindow: 128000, provider: ProviderType.OPENAI },
  { name: "gpt-3.5-turbo", model: "gpt-3.5-turbo", type: ModelType.CLOUD, description: "OpenAI GPT-3.5 Turbo", supportsTools: true, contextWindow: 16385, provider: ProviderType.OPENAI },
];

// Anthropic æ¨¡å‹
export const ANTHROPIC_MODELS: ModelConfig[] = [
  { name: "claude-sonnet", model: ANTHROPIC_MODEL_NAME, type: ModelType.CLOUD, description: `Anthropic - ${ANTHROPIC_MODEL_NAME}`, supportsTools: true, contextWindow: ANTHROPIC_MODEL_CONTEXT_LENGTH, provider: ProviderType.ANTHROPIC },
  { name: "claude-3-5-sonnet", model: "claude-3-5-sonnet-20241022", type: ModelType.CLOUD, description: "Anthropic Claude 3.5 Sonnet", supportsTools: true, contextWindow: 200000, provider: ProviderType.ANTHROPIC },
  { name: "claude-3-opus", model: "claude-3-opus-20240229", type: ModelType.CLOUD, description: "Anthropic Claude 3 Opus", supportsTools: true, contextWindow: 200000, provider: ProviderType.ANTHROPIC },
  { name: "claude-3-haiku", model: "claude-3-haiku-20240307", type: ModelType.CLOUD, description: "Anthropic Claude 3 Haiku", supportsTools: true, contextWindow: 200000, provider: ProviderType.ANTHROPIC },
];

// æ‰€æœ‰å¯ç”¨æ¨¡å‹
export const ALL_MODELS: ModelConfig[] = [...LOCAL_MODELS, ...CLOUD_MODELS, ...OPENROUTER_MODELS, ...OPENAI_MODELS, ...ANTHROPIC_MODELS];

// æ”¯æŒå·¥å…·è°ƒç”¨çš„æ¨¡å‹
export const TOOL_CAPABLE_MODELS = ALL_MODELS.filter((m) => m.supportsTools);

// é»˜è®¤æ¨¡å‹ï¼ˆæ ¹æ® USE_PROVIDER é€‰æ‹©ï¼‰
export const DEFAULT_MODEL = process.env.DEFAULT_MODEL || (USE_PROVIDER === ProviderType.OPENROUTER ? "openrouter" : "gpt-oss");

// æ ¹æ®åç§°è·å–æ¨¡å‹é…ç½®
// ä¼˜å…ˆåŒ¹é…å½“å‰ USE_PROVIDER ä¸‹çš„æ¨¡å‹ï¼Œé¿å…è·¨ Provider çš„åç§°å†²çª
export function getModelConfig(name: string): ModelConfig | undefined {
  // å…ˆåœ¨å½“å‰ Provider çš„æ¨¡å‹ä¸­æŸ¥æ‰¾
  const currentProviderModels = ALL_MODELS.filter(m => m.provider === USE_PROVIDER);
  const matchInProvider = currentProviderModels.find((m) => m.name === name || m.model === name);
  if (matchInProvider) {
    return matchInProvider;
  }

  // å¦‚æœå½“å‰ Provider æ²¡æœ‰è¿™ä¸ªæ¨¡å‹ï¼Œåˆ›å»ºåŠ¨æ€é…ç½®
  // è¿™å…è®¸ç”¨æˆ·ä½¿ç”¨ä»»æ„æ¨¡å‹åç§°ï¼Œå¹¶è‡ªåŠ¨ç»‘å®šåˆ°å½“å‰ Provider
  return {
    name,
    model: name,
    type: ModelType.CLOUD,
    description: `${USE_PROVIDER} - ${name}`,
    supportsTools: true,
    contextWindow: DEFAULT_CONTEXT_WINDOW,
    provider: USE_PROVIDER,
  };
}

// æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒå·¥å…·è°ƒç”¨
export function supportsToolCalling(name: string): boolean {
  const config = getModelConfig(name);
  return config?.supportsTools ?? false;
}

// è·å–æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£å¤§å°
export function getModelContextWindow(name: string): number {
  const config = getModelConfig(name);
  return config?.contextWindow ?? DEFAULT_CONTEXT_WINDOW;
}

// åˆ—å‡ºæ‰€æœ‰æ¨¡å‹
export function listModels(): void {
  ui.system(`\nå½“å‰ Provider: ${USE_PROVIDER}`);
  ui.system(`Ollama Host: ${OLLAMA_HOST}`);
  ui.system(`Ollama Cloud Host: ${OLLAMA_CLOUD_HOST}`);
  ui.system(`Ollama Cloud API Key: ${OLLAMA_CLOUD_API_KEY ? "å·²é…ç½® âœ“" : "æœªé…ç½® âœ—"}`);
  ui.system(`OpenRouter API Key: ${OPENROUTER_API_KEY ? "å·²é…ç½® âœ“" : "æœªé…ç½® âœ—"}`);
  ui.system(`OpenAI API Key: ${OPENAI_API_KEY ? "å·²é…ç½® âœ“" : "æœªé…ç½® âœ—"}`);
  ui.system(`Anthropic API Key: ${ANTHROPIC_API_KEY ? "å·²é…ç½® âœ“" : "æœªé…ç½® âœ—"}`);
  ui.system(`é»˜è®¤æ¨¡å‹: ${DEFAULT_MODEL}`);

  ui.heading("æœ¬åœ°æ¨¡å‹ (Ollama)");
  LOCAL_MODELS.forEach((m) => {
    const toolIcon = m.supportsTools ? "ğŸ”§" : "  ";
    ui.listItem(`${toolIcon} ${m.name.padEnd(18)} - ${m.description || m.model}`);
  });

  ui.heading("äº‘ç«¯æ¨¡å‹ (Ollama Cloud)");
  CLOUD_MODELS.forEach((m) => {
    const toolIcon = m.supportsTools ? "ğŸ”§" : "  ";
    ui.listItem(`${toolIcon} ${m.name.padEnd(18)} - ${m.description || m.model}`);
  });

  ui.heading("OpenRouter æ¨¡å‹");
  OPENROUTER_MODELS.forEach((m) => {
    const toolIcon = m.supportsTools ? "ğŸ”§" : "  ";
    ui.listItem(`${toolIcon} ${m.name.padEnd(18)} - ${m.description || m.model}`);
  });

  ui.heading("OpenAI æ¨¡å‹");
  OPENAI_MODELS.forEach((m) => {
    const toolIcon = m.supportsTools ? "ğŸ”§" : "  ";
    ui.listItem(`${toolIcon} ${m.name.padEnd(18)} - ${m.description || m.model}`);
  });

  ui.heading("Anthropic æ¨¡å‹");
  ANTHROPIC_MODELS.forEach((m) => {
    const toolIcon = m.supportsTools ? "ğŸ”§" : "  ";
    ui.listItem(`${toolIcon} ${m.name.padEnd(18)} - ${m.description || m.model}`);
  });

  ui.info("ğŸ”§ = æ”¯æŒå·¥å…·è°ƒç”¨");
  ui.newline();
}
